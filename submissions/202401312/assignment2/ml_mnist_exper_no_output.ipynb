{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUWlmB_UTuXz"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 임포트\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# MLP 모델 정의\n",
        "import sys\n",
        "sys.path.append('/content/')\n",
        "from utils import MLP, MLP_Tanh, MLP_Sigmoid, DataLoader_exper\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 128        # 배치 크기\n",
        "test_batch_size = 1000  # 테스트 배치 크기 (메모리 효율을 위해 크게 설정)\n",
        "learning_rate = 1e-3    # 학습률 (0.001)\n",
        "nb_epochs = 3           # 에포크 수\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 데이터 로딩 및 전처리\n",
        "train_loader, test_loader = DataLoader_exper(batch_size, test_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복되는 실험 코드를 함수로 묶기 (4.2 부분 활용)\n",
        "\n",
        "def run_experiment(experiment_name, model_class, learning_rate, nb_epochs=3):\n",
        "    print(f\"\\n=== {experiment_name} ===\")\n",
        "\n",
        "    # model, criterion, optimizer 준비\n",
        "    model = model_class().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # 훈련 루프\n",
        "    for epoch in range(nb_epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            imgs, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # 평가 루프\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            imgs, labels = batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    # 최종 테스트 정확도\n",
        "    final_accuracy = 100 * correct_test / total_test\n",
        "    print(f\"최종 테스트 정확도: {final_accuracy:.2f}%\")\n",
        "    return final_accuracy"
      ],
      "metadata": {
        "id": "dmaL-77pfHR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실험 1-1 : [하이퍼파라미터 튜닝] 학습률 증\n",
        "\n",
        "exper1_2 = [0.002, 0.004, 0.006, 0.008, 0.01, 0.1]\n",
        "\n",
        "result_exper1_2 = { }\n",
        "\n",
        "print(\"실험 1-1: 학습률 증가\")\n",
        "\n",
        "for lr in exper1_2:\n",
        "      acc = run_experiment(\n",
        "        experiment_name=f\"학습률 {lr:.3f}\",\n",
        "        model_class=MLP,\n",
        "        learning_rate=lr,\n",
        "        nb_epochs=3\n",
        "    )\n",
        "      result_exper1_2[f\"{lr}\"] = acc"
      ],
      "metadata": {
        "id": "HOB2qx3ZkKcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실험 1-2 : [하이퍼파라미터 튜닝] 학습률 감소\n",
        "\n",
        "exper1_2 = [0.0009, 0.0007, 0.0005, 0.0003, 0.0001, 0.00001]\n",
        "\n",
        "result_exper1_2 = { }\n",
        "\n",
        "print(\"실험 1-2: 학습률 감소\")\n",
        "\n",
        "for lr in exper1_2:\n",
        "      acc = run_experiment(\n",
        "        experiment_name=f\"학습률 {lr:.5f}\",\n",
        "        model_class=MLP,\n",
        "        learning_rate=lr,\n",
        "        nb_epochs=3\n",
        "    )\n",
        "      result_exper1_2[f\"{lr}\"] = acc"
      ],
      "metadata": {
        "id": "hsMP8_-GfML-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실험 2 : [모델 구조 개선] 다른 활성화 함수 사용\n",
        "\n",
        "print(\"실험 2: 다른 활성화 함수 사용\")\n",
        "\n",
        "exper2 = [\n",
        "    (\"Tanh 활성화 함수\", MLP_Tanh),\n",
        "    (\"Sigmoid 활성화 함수\", MLP_Sigmoid)\n",
        "]\n",
        "\n",
        "result_exper2 = { }\n",
        "\n",
        "for name, model_class in exper2:\n",
        "  acc = run_experiment(\n",
        "        experiment_name=f\"{name} 사용\",\n",
        "        model_class=model_class,\n",
        "        learning_rate=1e-3,\n",
        "        nb_epochs=3\n",
        "  )\n",
        "\n",
        "  result_exper2[name] = acc"
      ],
      "metadata": {
        "id": "YD1nnDgjlnKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}