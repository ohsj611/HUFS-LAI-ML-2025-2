```markdown
# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 96.98%
- 훈련 시간: 47초

## 실험 결과
### 실험 1: 학습률 변경
- 변경사항: 학습률 변경 실험은 학습률 증가 실험과 학습률 감소 실험으로 나누어 진행했다. 학습률 증가 실험에서는 학습률을 점진적으로 증가(0.002,0.004,0.006,0.008,0.01)하는 실험과 급진적으로 증가(0.1)하는 실험을 수행했다. 학습률 감소 실험에서도 학습률을 점진적으로 감소(0.0009,0.0007,0.0005,0.0003,0.0001)하는 실험과 급진적으로 감소(0.00001)하는 실험을 수행했다.
- 결과: 학습률 증가 실험은 (0.002,97.08%), (0.004,97.22%), (0.006,96.50%), (0.008,96.65%), (0.010,95.82%), (0.100,45.92%)의 결과를 보였고, 학습률 감소 실험은 (0.00090,96.96%), (0.00070,96.73%), (0.00050,96.22%), (0.00030,95.18%), (0.00010,92.82%), (0.00001,85.01%)의 결과를 보였다.
- 분석: 학습률 증가 실험의 경우, 학습률이 0.002일 때와 0.004일 때 기존 모델보다 향상된 성능을 보였으며, 특히 0.004일 때 가장 많이 향상된 성능을 보였다. 하지만 0.006부터는 기존 모델에 비해 전반적으로 성능이 저하되는 양상을 보였다. 학습률 감소 실험의 경우, 모든 학습률에서 기존 모델보다 성능이 저하되었고,학습률이 감소함에 따라 점차 성능이 지속적으로 저하되었다. 또한, 점진적으로 학습률을 변경했을 때는 성능 변화가 크지 않았지만, 급진적으로 학습률을 변경했을 때는 증가와 감소 모두 성능이 크게 저하되었다. 이러한 실험 결과는 모델에 적합한 학습률보다 크거나 작은 학습률이 수렴을 방해하기 때문에 성능이 저하된다고 해석할 수 있다.

### 실험 2: 활성화 함수 변경
- 변경사항: 기존 ReLU 활성화 함수를 Tanh 활성화 함수와 Sigmoid 활성화 함수로 변경하여 각각 실험을 진행했다.
- 결과: Tanh 활성화 함수는 96.33%, Sigmoid 활성화 함수는 95.24%의 결과을 보였다.
- 분석: Tanh 활성화 함수와 Sigmoid 활성화 함수 모두 기존 모델보다 약간 저하된 성능을 보였다. 이러한 실험 결과는 Vanishing Gradinet 문제 때문에 성능이 저하된다고 해석할 수 있다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 실험 결과를 바탕으로 하면, 학습률은 0.0.004 변경하고, 기존의 ReLU 활성화 함수를 유지하는 것이 가장 좋은 성능을 보이는 방법이다.
- 관찰된 패턴: 학습률이 커지면 성능이 향상되었지만, 일정 수준을 넘어가면 성능이 하락하였으며, 학습률이 작아지면 성능이 하락하였다. 활성화 함수를 ReLU에서 Tanh 또는 Sigmoid로 변경할 경우 모두 성능이 하락하였다.
- 추가 개선 아이디어: 에포크를 overfitting이 일어나지 않을 정도로 증가시키면 성능이 향상될 것이라 예상한다. 또한 조사해본 결과, Leaky ReLU, GELU, ELU와 같은 활성화 함수를 사용하면 성능을 미세하게 향상시킬 수 있을 것 같다.
```